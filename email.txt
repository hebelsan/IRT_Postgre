
Hätten Sie nächste Woche Zeit für ein Gespräch um den weiteren Verlauf und meine Ergebnisse zu bersprechen?


Hier ein Statusupdate meiner Erkenntnisse:
Ich arbeite derzeit an mehreren Ansätzen (unterschiedliche Datenbankschemen)

1. Ansatz - Möglichst alles mit TS_Vector ohne Wortmatrix
Mit diesem Ansatz habe ich mich in letzter Zeit am meisten beschäftigt.
Besonders mit der Ranking Funktion von TS_Vectoren.
Diese kann anhand der Anzahl, des Gewichts und des Abstands der gesuchten Wörter zueinander einen Ranking Wert ermitteln.
Dabei spielt es keine Rolle an welcher Stelle im Text die Wörter vorkommen, solange der Abstand zueinander gleich bleibt (Möglichkeit zur Verbesserung).
Trotz vieler Erweiterungen mit eigens geschriebenen C-Funktionen stoße ich immer wieder an die Limits des TS_Vectors (Der TS_Vector ist nicht wirklich für diese Art von Anwendung gemacht).
Da ich die Wikiseiten in einzelne Abschnitte unterteile, starten die Indexe für jeden Abschnitt wieder bei 0.
Aus diesem Grund habe ich eine eigene C-Funktion geschrieben, der man einen Offset bei der Erstellung eines TS_Vectors mitgeben kann.
Somit konnten die korrekten Positions Indexe der Lexeme im TS_Vector erhalten bleiben.
In meiner Datenbank befinden sich derzeit 2000 Wikiseiten zur Kategorie 'Sports'.
Das längste Dokument hat 124169 Wörter.
Der TS_Vector erlaubt jedoch nur einen maximalen Wert von 16383 für die Position, da nur 20 bits für das Positionsarray zu Verfügung stehen.
Selbst wenn ich nur die Lexeme zähle sind es in meinem Datensatz von 2000 Wikiseiten immer noch maximal 19603. Also zu viele, als dass sie in das Positionarray passen würden.
Deshalb bin ich wieder zurückgerudert zu dem Stand, dass jeder Abschnitt beim Index 0 beginnt.
Was ich derzeit machen kann ist ein Ranking für jeden Abschnitt zu berechnen.
Dieser basiert auf dem Abstand der gesuchten Wörter in dieser Sektion, der Häufigkeit und des Gewichts (A=PageTitle,B=SectionTitle,C=SectionText).
Anschließend addiere ich die Rankingwerte der Abschnitte und dividiere sie durch die Gesamtanzahl der Wörter in diesem Dokument.
Somit erhält jedes Dokument einen Rankingwert.
Dieser Ansatz funktioniert sogar für And, OR, Not etc., da die standard ts_query benutzt wird.
Limitiert ist das ganze durch die maximale Anzahl von 19603 Lexemen pro Abschnitt, was in meinem Fall deutlich ausreicht.
Verbessern könnte man diesen Ansatz indem man für jeden Abschnitt einen Offset speichert um die Originalindexe wieder herstellen zu können.
Jedoch müsste dann auch eine custom Rankingfunktion geschrieben werden.

2. Ansatz - Kombination aus TS_Vector und Wortmatrix
In diesem Ansatz wird der TS_Vector nur verwendet, um zu checken ob das gesuchte Wort auf einer Wikiseite vorkommt oder nicht.
Dadurch könnte man immer noch den Vorteil der GIN-Indexierung nutzen.
Mit den erhaltenen Page_ids, kann in einer Wort_matrix Position und Häufigkeit identifiziert werden.
Meine Idee ist, dass für jede Wikiseite eine Wortmatrix-Tabelle erstellt wird.
Dabei weis ich noch nicht wirklich ob das so realisierbar ist, bzw. wie sich das auf die Performance auswirken wird.
Eine kurze recherche hat ergeben, dass die maximale Anzahl an Tabellen von der maximalen Anzahl an Dateien 'cat /proc/sys/fs/file-max' abhängt, welche wiederum vom gegebenen RAM abhängt.

3. Nur Wortmatrix ohne TS_Vectoren
Beim letzten Ansatz wird das ganze Vokabular (Alle Lexeme) in einer großen Tabelle gespeichert.


Des Weiteren habe ich großes Interesse an dem von Ihnen in diesem Semester angebotenen Seminar "Topics and Trends in Text Analytics".
Leider ist die Müsligruppe schon voll.
Gibt es trotzdem eine Möglichkeit an diesem Seminar teilzunehmen, besonders da es mir bei meiner Masterarbeit helfen könnte?
